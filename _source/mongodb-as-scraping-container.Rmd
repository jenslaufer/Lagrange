---
layout: post
title: 'MongoDB GridFS as your Web Scraping Filesystem'
categories:
  - data analysis
show_comments: true
tags: Python, MongoDB, Scraping
image: missing_values.jpg
output: 
  html_document: default
---

> "Data is the new oil. It's valuable, but if unrefined it cannot really be used. It has to be changed into gas, plastic, chemicals, etc to create a valuable entity that drives profitable activity; so must data be broken down, analyzed for it to have value." 
-- <cite>Clive Humby</cite>

There are cases when your only option is to scrape data from external websites. This data is first unrefinded and we chnge into gas. The first step before you scrape any website is to check their _Terms of Service_ and the robots.txt of the website what's allowed or not. In this article I want to show you how to use MongoDB gridfs for persisting data you are  scraping. 

## Scraping process

The first approach when writing a scraper is that your fetch the data from the website and your parse the data immediatly. You often need certain features/variables from the website

This approach has it's disadvantages:

1. You often need a few data features in the first step, but you notice later that you need more stuff. You need scrape the website then again for the raw HTML page.
2. As a developer we have a kind trial-and-error programming style. We write a bit of code, we test it, we making coding errors. This approach is not so good with scraping, as you risk already to be blocked if you fetch the same page over and over during development already.

To avoid these pitfalls a proven tactic is to fetch the the raw data from the website just once (and maybe later in future when it might be outdated). The first step is therefore to write just dump code that just fetches the website content you need. You are checking for the URL parameters etc which are needed to get the raw data you need. So you write code that builds the URl in the first step. Then you take the generated URL and check in your browser if you build it the right way. Then you try to fetch data with your code. You need to check the headers etc. 
If you thta is stuff all ok you save the raw data somewhere. You first guess to just save the data somewhere. 


## Scraping to the filesystem

The local filesystem is the obvious way to save the data. You need a

## Scraping to GridFS





Normally it's a good approach to scrape the raw files and keep them for later. You can do the exraction of features in a second step. The scraping of the raw files is normlly easy to implement.
Also you want to avoid to scrape a lot of files from a website over and over to avoid to get blocked. Maybe you need just a portion of data, but later you need more stuff.
You don't need to download the file again.

When scraping raw files into the local filesystem there are multiple disadvantages:

- Yo are not able to persist meta information along with the file  
- If you want to containerize the Scraper apllication is better to have a stateless container
- number of files are limited per directory
- not multiple versions of same files
- Another big plus is that if we use the ordinary filesystem, we would have to handle backup/replication/scaling ourselves
- Last but not least, we can keep information associated with the file (who has edited it, download count, description, etc.) right with the file itself.

Therefore it is a good ide to put the files into a gridfs container which is provided by MongoDB
- Files are compressed instead of 
- File can eassily served with an API+
- storing the files in gridfs is more efficient than storing them in the local filesystem
- You are able to have different versions of the file
- The files are replicated

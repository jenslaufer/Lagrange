---
title: "Example Use cases of Docker in the Data Science Process"
subtitle: 'or how to avoid the It-works-on-my-computer-but-nowhere-else-problem'
image: 'PnvK7v6.png'
output: html_document
layout: post
show_comments: yes
tags: docker, process, data science
categories: data science
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, message = F, warning = F)
```

```{r}
library(tidyverse)
```


This excellent [comic by Jeff Lofvers](http://donthitsave.com/comic/2016/07/15/it-works-on-my-computer) 
illustrates what you often face in software development but also in data science. You are preparing a data analysis or predictive model, 
but when you want to share it, it does not work on someone else machine. It fails, 
because libraries are missing, libraries have the wrong version ("dependency hell"), or configurations differ. Time-Consuming troubleshooting starts.

The solution is not far away: [Docker](https://www.docker.com/) solves the problem of reproducibility in a lightweight manner, but also offers you many other advantages. 

__What is Docker?__

Docker is a free software that performs operating-system-level virtualisation.
Docker is used to running software packages called containers. Containers are isolated from each other and bundle their application, tools, libraries and configuration files. All containers are run by a single operating system kernel and are thus more lightweight than virtual machines. [[Wikipedia on Docker](https://en.wikipedia.org/wiki/Docker_(software))]

Docker makes it easy to create, run and distribute applications. Applications are packaged up with everything that is needed to run the application. The concept guarantees that the container can be run on every docker runtime environment.

__Advantages of Docker__:

1. _Reproducibility_

    With Docker, you ensure that your software artefact (application, data analysis, model) runs on all docker runtime environment. Your shipments are more robust, as the container contains everything that's needed to run your artefact. You are not distributing only the code, but also the environment.

2. _Consisitency_

    Docker equips you with one uniform and consistent runtime environment for all kinds of software artefacts: Data Analyses, Machine Learning Models, Dashboard Applications, Web Applications, Static Websites, Machine Learning Model Trainings. It reduces the time for system administration and lets you focus on your core work. You might know Anaconda environments; Docker is something similar for the __whole software ecosystem__.

3. _Tracability:_ 

    a.) Version controlling of Docker container code

    A Docker container is built from a script which is a human-readable summary of the necessary software dependencies and environment. This script can be version controlled. The script is entirely traceable this way.


    b.) Uniform distribution environment for all artefacts

    Docker containers can be stored in a repository within your organisation. You keep the whole version history this way.

4. _Portability_

    Docker containers can easily be ported from one docker environment to another, with tools like docker-machine.  [Docker Swarm](https://docs.docker.com/engine/swarm/) or [Kubernetes](https://kubernetes.io/) let you scale applications automatically.


However, what are the use-cases for Docker in the data science universe? I will concentrate on the data science [OSEMN process](https://www.thelead.io/data-science/5-steps-to-a-data-science-project-lifecycle): 



![OSEMN: Data Science Process](https://www.thelead.io/wp-content/uploads/2019/01/data-science-process-OSEMN-framework.jpg)



## The use cases of Docker in the Data Science Process

Reality is today that the process consists of a wide variety of tools and programming languages. Docker is the go-to platform to manage these heterogenous technology stacks, as each container provides the runtime environment it needs to run exactly the one application it is packed around. The interference of technology stacks is reduced this way.

### 1. Obtain: Gather Data from relevant sources

Data is the oil for data science. You retrieve it from surveys, clinical trials, web scraping, scientific experiments, corporate applications or simulations. Typically data engineers are dealing with the data, but also stakeholders from other fields are involved, which leads to a wide diversity of database systems and programming languages.  

  - Web scraping: Python application with low-level dependencies to Selenium Chrome extension and a Postgres database is packed as a multi-container application with [Docker Compose](https://docs.docker.com/compose/)
  - Labelling images: Lean web application with Vue.js, a NodeJS backend and a MongoDB is used to label images
  - Surveys: Small static microsite build by the marketing team  in plain HTML with an integrated SurveyMonkey form that saves the data in CSV on the company's file server
  - Corporate application: Banking web application implemented in AngularJs and Java in the backend with an Oracle database produces valuable banking data from the customers
  - Computer simulation: Simulation programmed in C++ stores its results in JSON on Amazon S3 
  - Asynchronous data streams:  Sensors in cars sending their data to Kafka, which is distributing the data to all applications within the company which are interested in it

### 2. Scrub: Clean and aggregate data to formats the machine understands

The Data which was obtained in Step 1 is the oil, but right now it's raw oil. You need to clean, process and combine it to the data you need for analysis and modelling.
  - Aggregation: An Application in Java gets the data from Kafka streams, does aggregations on the low-level data and stores it to an Oracle database
  - Data analysts clean and preprocess the data from a corporate web application as preparation for answering a business question with an RMarkdown Notebook, which they want to share with the management
  - Machine Learning engineer combines data from different data sources, cleans and preprocesses data for a predictive model in a Juypter Notebook 
  - Data is combined, cleaned, aggregated and preprocessed and persisted for interactive dashboard applications high level in Tableau
  

### 3. Explore: Find patterns and trends

In the exploration phase, all you have to do is understand what patterns and values are in the hands of the data. You want to make the results available to everyone interested.


  - Data Analysts are creating Jupyter or RMarkdown Notebooks to answer a question they need to share with everyone interested in it.
  - Data Analyst cluster the customers into new segments based on data analysis which is persisted in a new Customer Segment MySQL Database 
  - Data Analysts build interactive web applications for high-level data exploring for interested stakeholders in R Shiny, Dash, Tableau or Kibana
  

### 4. Model: Construct models to predict and forecast

The cleaned and preprocessed data is  used to train machine or deep learning algorithms. You create models this way which are a mathematical representation of observed data.

- The complete training process for a neural network for object detection on images is isolated to a Docker container, which is run on a GPU-enhanced cloud instance ([Nvidia Docker](https://github.com/NVIDIA/nvidia-docker)). The instance is runnable on Azure, Google Cloud, AWS or Alibaba cloud with no effort.
- A Keras model is imported into DeepLearning4J and published as a Java Microservice due to performance issues with Python

### 5. Interpret: Put the results into good use

The data science insights are communicated and visualised. Models are distributed as microservices.

- Static website to provide data story articles
- Microsites to tell the data story
- A predictive machine learning model in Python is released as microservice
- A REST microservice  in Java with aggregated data is released to paying B2B customers
- A product recommender Service in Python is integrated into the company's web application
- Data-driven stories are  published on the company's Tableau Server and shared for internal and external use
- Interested insights from data analysts are shared by data storytellers in the content management team to the public on the static Jekyll company blog


## Conclusion

Docker is a powerful tool also for data scientists and can be applied to all stages in the OSEMN process. You can provide all kind of applications in a consistent, reproducible and traceable way. The applications can be very different in their technology stack, which is the reality in data science projects. Data engineers work with databases like Oracle, MySQL, MongoDB, Redis or ElasticSearch or programming languages like Java, Python or C++. In the analytics and modelling team, people might work with R, Python, Julia or Scala, while data storytellers tell their story with d3.js in JavaScript or use Tableau. As specialists are rare, it's better to let the people work with the stuff they know instead of them into specific technology stacks they are not familiar with. You get better results faster. Docker is the way to go to manage this heterogeneous technology landscape in data science.

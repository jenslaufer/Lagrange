---
title: "Practical example of Training a Neural Network on AWS with Docker"
subtitle: "Training a shallow neural network on top of an InterceptionV3 model on CIFAR-10 within Docker on a GPU-instance on AWS"
output: 
  html_document:
     highlight: haddock
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

My last article [Example Use Cases of Docker in the Data Science Process](https://jenslaufer.com/data/science/use-cases-of-docker-in-the-data-science-process.html)  was about Docker in Data Science in general. This time I want to get my hands dirty with a practical example. 

In this case study, I want to show you how to train a shallow neural network on top of a deep InterceptionV3 model on CIFAR-10 images within a Docker container. I am using a standard technology stack for this project with Python, Tensorflow and Keras.  The [source code for this project](https://github.com/jenslaufer/neural-network-training-with-docker) is available on Github. 

What you will learn in this case study:

- Setup of GPU empowered cloud instance on AWS from your command line with docker-machine
- Usage of a tensorflow docker image in your Dockerfile
- Setup of multi-container Docker application for training a neural network with docker-compose
- Setup of a MongoDB as Persistence container for training meta-data and file storage for models
- Transfer learning of convolutional neural network


Let's define the requirements for this little project:

- Training must be done in the cloud on a GPU empowered instance in AWS
- Flexibility to port the whole training pipeline also to Google Cloud or Microsoft Azure 
- Usage of nvidia-docker to activate the full GPU-power on the cloud instance
- Encapsulation of the training pipeline in the Docker container
- The different models must be persisted with all models metadata on MongoDB for model versioning and reproducibility. 
- Usage of docker-compose for the multi-container application (training container + MongoDB)
- Usage of docker-machine to manage the cloud instance and start the training from the local command line with docker-compose

__Input__

The Docker container is parameterised from a MongoDB collection with all parameters for a training session. 

- loss function
- optimiser
- batch size
- number of epochs
- subset percentage of all samples used for training (used for testing the pipeline with fewer images)

I use just a subset of possible training parameters for simplicity.

![Studio 3T Screenshot Input Parameters Files](https://res.cloudinary.com/jenslaufer/image/upload/c_scale,w_1713/v1555506062/screenshot_input.png)

__Output__

- Model architecture file
- Model weights file
- Model accuracy on the test set

![Studio 3T Screenshot Output Files](https://res.cloudinary.com/jenslaufer/image/upload/c_scale,w_1727/v1555506062/screenshot_output.png)

The advantage of this setup that you are persisting every model you train with its parameters and performance on the test set. You can use the persistence layer for further steps in your machine learning pipeline. You can, e.g. load the best performing model in a production environment or run further analyses for parameter tuning. 


### 1. Prerequisites

1. Installation of [Docker](https://docs.docker.com/install/) along with [Docker Machine](https://docs.docker.com/machine/) and [Docker Compose](https://docs.docker.com/compose/) (On Mac and Windows all tools are installed with the standard docker installation)
2. Creation of an Account on [AWS](https://aws.amazon.com)
3. Installation and Setup of the [AWS Command Line Client](https://github.com/aws/aws-cli)


### 2. Training Script


I put the whole training pipeline into one script [src/cnn/cifar10.py](https://github.com/jenslaufer/neural-network-training-with-docker/blob/c045323c372bb46535f563c456117a8befa4b05f/src/cnn/cifar10.py) It consists of one class for the whole training pipeline:

1. Downloading of the CIFAR-10 images to the container file system.
2. Loading of the base model (InceptionV3) with imagenet weights and removal of the top layer
3. Extracting of bottleneck features for training and test images; persisting of the features in MongoDB for further usage.
4. Creation and Compilation of the shallow neural network; persisting of model architecture in MongoDB 
5. Training of the shallow model; persisting of model weights in MongoDB
6. Model testing on test set; persisting of accuracy metric in MongoDB




### 3. Containerization


#### a.) Dockerfile

Everything you need to train the neural network I put into the Dockerfile, which defines the runtime environment for the training.



```{.Docker .numberLines} 
FROM tensorflow/tensorflow:1.13.1-gpu-py3 

COPY src /src

WORKDIR /src

RUN pip install -r requirements.txt

ENV PYTHONPATH='/src/:$PYTHONPATH'

ENTRYPOINT [ "entrypoints/entrypoint.sh" ]
```



_Line 1: Definition of the base image. The setup and configuration are inherited from this image. An official tensorflow image with python3 and GPU support is used._

_Line 3: Everything in the local directory src, like the training script and entry point, is copied into the Docker image._

_Line 5: Container is started in src directory_

_Line 7: Installation of python requirements_

_Line 9: src directory is added to PYTHONPATH to tell python to look for modules in this directory_

_Line 11: Definition of the entry point for the image. This entry point script is executed when the container is started. This script starts our python training script._


The entry point shell script is pretty self-explaining. It starts the python module with no parameters. The module fetches the training parameter from the MongoDB on startup.



```bash
#!/bin/bash

python -m cnn.cifar10
```


#### b.) Docker Container Build

First, I need to build the docker image. You can skip this step as I shared the ready-built [Docker image on Docker Hub](https://hub.docker.com/r/jenslaufer/neural-network-training-with-docker/tags). The image is automatically downloaded when it is referenced the first time.

```bash
docker build -t jenslaufer/neural-network-training-with-docker .
```


#### c.) Multicontainer Application

I have two docker containers in my setup: The Docker container for training and a MongoDB for persisting meta-data and as a file server.

You use docker-compose for this scenario. You define the containers that make your application in a docker-compose.yml

```{.yaml .numberLines} 
version: '2.3'

services:
  training:
    image: jenslaufer/neural-network-training-with-docker:0.1.0-gpu
    container_name: neural-network-training-with-docker
    runtime: nvidia
    depends_on:
      - trainingdb

  trainingdb:
    image: mongo:3.6.12
    container_name: trainingdb
    ports:
      - 27018:27017
    command: mongod
```

_Line 4-5: Definition of the training container which uses the jenslaufer/neural-network-training-with-docker image with tag 0.1.0-GPU. This image is automatically downloaded from the public Docker Hub repository_

_Line 7: The runtime environment for tensorflow_

_Line 9: The training container needs the trainingdb container for execution. In the code, you use mongodb://trainingdb as Mongo URI_

_Line 11-12: Definition of the MongoDB database. An official mongo image from Docker Hub is used with version 3.6.12_

_Line 14-15: The internal port 27017 is available at port 27018 from outsite_

_Line 16: Mongo daemon is started_

You can see that it's straightforward to set up a multi-application with docker compose â€” you just set up a database with a few lines of code without complicated installation routines.



### 4. AWS 


We want to train the neural network on the AWS. This step takes the most effort. However, once the AWS instance is set up, you can reuse it.



```bash
docker-machine create --driver amazonec2 --amazonec2-instance-type p2.xlarge --amazonec2-ami ami-0891f5dcc59fc5285 --amazonec2-vpc-id <YOUr VPC-ID> cifar10-deep-learning
```
We create a AWS instance with Docker Machine. I use a Amazon Image with Ubuntu 18.04 (ami-0891f5dcc59fc5285) which has CUDA 10.1 and nvidia-docker already installed. 
I use the p2.xlarge instance type, which is cheapest GPU instance. The p2.xlarge uses one Tesla K80 GPU. You have to find out your VPC-ID for the setup.

Use this command to find out  the VPC-ID:
```bash
aws ec2 describe-vpcs --filters "Name=isDefault, Values=true"
```
You can get the VPC-ID as well from the [web console](https://console.aws.amazon.com/vpc/home?region=us-east-1#vpcs:sort=VpcId)


For more information check the [Docker Machine with AWS](https://docs.docker.com/machine/get-started/) Documentation.


__*Warning: The p2.xlarge cost $0.90 per HOUR. Please don't forget to stop the instance after completing your training sessions*__



### 5. Training the neural network

To ensure that the docker commands are going against our AWS instance we need to execute this command:

```bash
docker-machine env cifar10-deep-learning

```

Afterwards, you can list your machines
```bash
docker-machine ls
```

```bash
NAME                    ACTIVE   DRIVER      STATE     URL                      SWARM   DOCKER     ERRORS
cifar10-deep-learning   *        amazonec2   Running   tcp://3.83.26.763:2376           v18.09.5
```

Ensure that you see the star for the active environment. It's the environment against all docker commands are executed.
Keep in mind that you execute the commands in your local shell. It's very convenient.

We can now train the neural network. 


```bash
docker-compose -f docker-compose-gpu.yml up -d
```
All the docker images onto the AWS instance start the training container and the MongoDB.

You can check the logs of the neural network container.
```bash
docker logs -f neural-network-training-with-docker
```
You won't see too much, as we didn't add any training sessions into the DB.


Let's add training session parameters.

We log into MongoDB container for this:

```bash
docker exec -it trainingdb bash
```


We open the mongo client. We are selecting the DB 'trainings' with use command. We add then training session with on 5% of the images a rmsprop Optimizer with a
batch size of 50 and 20 epochs. 
```bash

root@d27205606e59:/# mongo
MongoDB shell version v3.6.12

> use trainings
switched to db trainings
> db.sessions.insertOne({"loss" : "categorical_crossentropy", "subset_pct" : 0.05, "optimizer" : "rmsprop", "batch_size" : 50.0, "epochs": 20})
{
        "acknowledged" : true,
        "insertedId" : ObjectId("5cb82c7e552612f42ba7831b")
}
```

We leave the MongoDB and restart the containers:

```bash
docker-compose -f docker-compose-gpu.yml up -d

```

You can see more logging this time with docker log command.


```bash
docker logs -f neural-network-training-with-docker
```

We log again  into the MongoDB:
```bash

root@d27205606e59:/# mongo
MongoDB shell version v3.6.12

> use trainings
switched to db trainings

> db.sessions.find({"_id": ObjectId("5cb82c7e552612f42ba7831b")})
{
        "_id" : ObjectId("5cc01ab927d7bcb89d69ab58"),
        "loss" : "categorical_crossentropy",
        "subset_pct" : 0.05,
        "optimizer" : "rmsprop",
        "batch_size" : 50,
        "epochs" : 20,
        "test_sample_size" : 500,
        "train_sample_size" : 2500,
        "accuracy" : 0.7900000009536743,
        "date" : ISODate("2019-04-24T08:15:09.256Z")
}
>
```

You can see that our training got an accuracy of 75% on the test set.

```bash
root@f070523a5d05:/# mongo
MongoDB shell version v3.6.12
> use trainings
switched to db trainings
> db.fs.files.find()
{
        "_id" : ObjectId("5cc01afc0127a900099528e1"),
        "length" : 184320250,
        "contentType" : "application/x-hdf",
        "type" : "bottleneck_features",
        "md5" : "332b66dd8bcbb3134ca8336da4fee5e2",
        "filename" : "cifar10_bottleneck_features_2500_train.npz",
        "uploadDate" : ISODate("2019-04-24T08:14:54.405Z"),
        "chunkSize" : 261120
}
{
        "_id" : ObjectId("5cc01b000127a90009952ba4"),
        "length" : 36864250,
        "contentType" : "application/x-hdf",
        "type" : "bottleneck_features",
        "md5" : "b237f378da82e64c13059ade793f5778",
        "filename" : "cifar10_bottleneck_features_500_test.npz",
        "uploadDate" : ISODate("2019-04-24T08:14:57.205Z"),
        "chunkSize" : 261120
}
{
        "_id" : ObjectId("5cc01b030127a90009952c33"),
        "length" : 3298104,
        "contentType" : "application/x-hdf",
        "type" : "model_arch",
        "md5" : "9fd27e4c8fdca43c89709f144547dfe8",
        "session_id" : ObjectId("5cc01ab927d7bcb89d69ab58"),
        "filename" : "model_arch.hdf5",
        "uploadDate" : ISODate("2019-04-24T08:14:59.399Z"),
        "chunkSize" : 261120
}
{
        "_id" : ObjectId("5cc01b0c0127a90009952c41"),
        "length" : 1621,
        "contentType" : "text/json",
        "chunkSize" : 261120,
        "type" : "training_history",
        "md5" : "ddaa898e428189af9a3c02865087ed79",
        "session_id" : ObjectId("5cc01ab927d7bcb89d69ab58"),
        "filename" : "training_history.json",
        "uploadDate" : ISODate("2019-04-24T08:15:08.371Z"),
        "encoding" : "utf-8"
}
{
        "_id" : ObjectId("5cc01b0c0127a90009952c43"),
        "length" : 6584512,
        "contentType" : "application/x-hdf",
        "type" : "model_weights",
        "md5" : "b8cd48c4b9f17b3230c6890faabc2aac",
        "session_id" : ObjectId("5cc01ab927d7bcb89d69ab58"),
        "filename" : "model_weights.hdf5",
        "uploadDate" : ISODate("2019-04-24T08:15:08.415Z"),
        "chunkSize" : 261120
}
```




### 6. Models Evaluation

We can compare the results from the different training sessions quickly with MongoDB. We also have all model parameters for all the sessions.

We can also use Pandas in Python or dplyr in R for further analysis.


The top 3 models by their accuracy:

```bash

root@f070523a5d05:/# mongo
MongoDB shell version v3.6.12
> use trainings
switched to db trainings

> db.sessions.find({"accuracy":{'$exists':1}}).sort({"accuracy":-1}).limit(4).pretty()
{
        "_id" : ObjectId("5cc03fa4f7d2acdfd7e1a452"),
        "loss" : "categorical_crossentropy",
        "subset_pct" : 0.5,
        "optimizer" : "sgd",
        "batch_size" : 50,
        "epochs" : 20,
        "test_sample_size" : 5000,
        "train_sample_size" : 25000,
        "accuracy" : 0.8282,
        "date" : ISODate("2019-04-24T11:05:56.743Z")
}
{
        "_id" : ObjectId("5cc03fa4f7d2acdfd7e1a450"),
        "loss" : "categorical_crossentropy",
        "subset_pct" : 0.5,
        "optimizer" : "rmsprop",
        "batch_size" : 50,
        "epochs" : 20,
        "test_sample_size" : 5000,
        "train_sample_size" : 25000,
        "accuracy" : 0.8044,
        "date" : ISODate("2019-04-24T10:59:40.469Z")
}
{
        "_id" : ObjectId("5cc03fa4f7d2acdfd7e1a451"),
        "loss" : "categorical_crossentropy",
        "subset_pct" : 0.5,
        "optimizer" : "adam",
        "batch_size" : 50,
        "epochs" : 20,
        "test_sample_size" : 5000,
        "train_sample_size" : 25000,
        "accuracy" : 0.7998,
        "date" : ISODate("2019-04-24T11:02:43.122Z")
}
{
        "_id" : ObjectId("5cc03fa4f7d2acdfd7e1a453"),
        "loss" : "categorical_crossentropy",
        "subset_pct" : 0.5,
        "optimizer" : "rmsprop",
        "batch_size" : 20,
        "epochs" : 20,
        "test_sample_size" : 5000,
        "train_sample_size" : 25000,
        "accuracy" : 0.7956,
        "date" : ISODate("2019-04-24T11:11:25.041Z")
}

```

You can download the Model Weights and Model Architecture for the best model or you. 





## Conclusion

We set up in this case study a GPU-empowered instance on AWS. We can use this new computer to train a neural network. 



### Improvements

- Load shallow model from MongoDB instead of hard-coded into the code
- More generic way to persist the validation metric (in this case accuracy)
- More generic way to persist optimiser and unique parameters for a certain optimiser
- Load the python module into the container in a generic way. 












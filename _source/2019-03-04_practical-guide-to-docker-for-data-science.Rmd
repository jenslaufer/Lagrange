---
title: "Use cases of Docker in the Data Science pipeline"
subtitle: 'or how to avoid the It-works-on-may-computer-but-nowhere-else-problem'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, message = F, warning = F)
```

```{r}
library(tidyverse)
```

![But it works on my machine](http://i.imgur.com/PnvK7v6.png)
This excellent [comic by Jeff Lofvers](http://donthitsave.com/comic/2016/07/15/it-works-on-my-computer) 
illustrates what you often face in software development but also in data science. You are preparing a data analysis or predictive model, 
but when you want to share it, it does not work on someone else machine. It fails, 
because libraries are missing, libraries have the wrong version ("dependency hell") or other devices are configured differently. Time-Consuming troubleshooting starts.

The solution is not far away: [Docker](https://www.docker.com/) solves the problem of reproducibility in a lightweight manner, but also offers you many other advantages. 

__What is Docker?__

Docker is a free software that performs operating-system-level virtualisation.
Docker is used to running software packages called containers. Containers are isolated from each other and bundle their application, tools, libraries and configuration files. All containers are run by a single operating system kernel and are thus more lightweight than virtual machines. [Wikipedia on Docker](https://en.wikipedia.org/wiki/Docker_(software))

Docker makes it easy to create, run, deploy and distribute applications using containers. Applications can be packaged up with all the parts it needs and shipped in one package. The concept ensures that the container can be run on other docker runtime environments.

__Advantages of Docker__:

1. _Reproducibility_

    With Docker, you can ensure that your application or analysis is runnable smoothly. Your shipments are more robust and reliable, as the container contains everything that's needed for the use. You are not distributing only the code, but also the environment the code can be run.

2. _Consisitency_

    Using docker containers lets you have one uniform and consistent runtime environment for all kinds of software artefacts: Data Analyses, Machine Learning Models, Dashboard applications, Web Applications, static websites, Machine Learning Model Training. It saves enormous time for system administration and lets you focus on your core work. You might know something from anaconda environments, but docker is beyond this for the whole software ecosystem

3. _Tracability:_ 

    a.) Uniform distribution environment for all artefacts

    Docker containers are stored in one uniform and consistent docker repository within your organisation with all its version history.

    b.) Version controlling of Docker container code

    Docker containers are built from scripts which can be version controlled. It provides a human-readable summary of the necessary software dependencies and environmental variables.

4. _Portability_

    Docker containers can easily be ported from one docker environment to another, with tools like docker-machine.  [Docker Swarm](https://docs.docker.com/engine/swarm/) or [Kubernetes](https://kubernetes.io/) provide you scalable runtime environments.


However, what are the use-cases for Docker in the data science universe? I will concentrate on the data science OSEMN workflow: 

__O__: Obtaining data

__S__: Scrubbing data

__E__: Exploring data

__M__: Moodeling

__N__: Interpreting data


## The use cases of Docker in Data Science

Docker can be used in all parts of the data science pipeline nd process. The different container may have completely different toolset and programming languages. The various applications, containers can be run on one unique paltform which is suitable to run docker containers.


- : Container for scraping data
- Container for Data Product like Shiny App
- Container with a static web site
- Container for training in machine learning training pipeline
- Conatiner wit a trained model

### 1. Obtaining data

Data may be generated from clinical trials, scientific experiments, surveys, web pages or computer simulations:

  - Application for scraping data and persisting the data in databases, which are shared in the data workflow
  - Web Application allowing mechanical turks to label data
  - Web application for surveys
  - Corporate web application used by the customers which generates data
  - Computer simulation that persists it's results into database
  
All these kinds of data obtaining applications can be packed into docker containers. They might not even in the hand of data science team and programmed in languages that are not part of the data science stack. They might use different kind of databases, which can easily set up with docker along with application and suits perfectly for the use case.

### 2. Scrubbing data

Scrubbing data refers to the preprocessing needed to prepare data for analysis. This may involve removing particular rows or columns, handling missing data, fixing inconsistencies due to data entry errors, transforming dates, generating derived variables, combining data from multiple sources

  - Applications that aggregate new data from raw data 
  - Applications that aggregate 
  - Application that clean data to ensure data  quality
  
These in containers, which This depends also what kind of technology backgorund the team has. Some might work with R, others with Python. You might have people that work with SQL or use aggregation framework from MongoDB. 
  

### 3. Exploring data

In the exploration phase, all you have to do is go understand what patterns and values are in the hands of the data. You want to make the results available to everyone who is interested.

  - Share notebooks with the results from EDA with docker containers in a reproducable way
  - Interactive Data Exploring Dashboard Apllications Shiny, Dash, Kibana, Tableau etc.
  

### 4. Modeling data

Now, we have come to a very interesting step. The so-called model is the general model in statistics. You can think of a machine learning model as a tool lying in your toolbox. You have a lot of algorithms to choose from, and you can use them to help achieve different business goals. The better the features you choose, the better your predictive ability will be. After cleaning up the data and extracting important features, we can use the model as a predictive tool to improve the effectiveness of business decisions.

Predictive Analytics has become the key to changing the rules of the game. What predictive analysis does is not to look back at the “what happened”, but to help managers answer questions such as “What will be next?” and “How should we respond?” (from Forbes Magazine).

- Container for training machine learning model
- Microservices whch provide machine learning model

### 5. Interpreting data

The results of 
The data science results are communicated and visualised and sti
Tell the stories with the insights you got from data. 

- Static website to provide data story articles
- Microsites  to tell data story


## Conclusion

Docker is a powerful tool also for data scientists and can be applied to all stages in the OSEMN pipeline. You can provide all kind of applications in a consistent, reproducible and traceable way. The applications can be very different in their technology stack, which is the reality in data science projects. There data engineers who work with databases like Oracle, MongoDB and programming languages like Java, Python or C++. In the analytics and modelling team people might work with R, Python, Julia or Scala, while data storytellers tell their story with d3.js in JavaScript. As specialists are rare, it's better to let work with the stuff they know instead of pushing people into specific technology stacks they are not familiar with. Docker helps with its consistent, portable and reliable runtime environment to manage this heterogeneous application. 

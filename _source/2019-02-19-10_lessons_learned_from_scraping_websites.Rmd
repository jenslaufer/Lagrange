---
layout: post
title: "10 Lessons Learned from Scraping Websites"
subtitle: "Valuable insights from retrieving data from many websites over the last years which I want to share with you"
categories:
  - data analysis
show_comments: true
tags: R, ggplot, dplyr, tidyverse, data vizualisation, data analysis, EDA
image: missing_values.jpg
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

> "Data is the new oil. It's valuable, but if unrefined it cannot really be used. It has to be changed into gas, plastic, chemicals, etc. to create a valuable entity that drives profitable activity; so must data be broken down, analysed for it to have value." - Clive Humby

A frequent criticism of Kaggle and MOOC (Massive Open Online Courses) is that they use pre-cleaned or non-real-world datasets. Data Science is more than that. Data retrieval and cleansing are essential parts of the process too. You and I know that data can make a difference. If you can get better data, you perform better. Do you think that is a task for a data engineer? In a small startup or as an entrepreneur following your passions, you don't have access to one. You don't have data at all. What is the solution to this problem? 

Scraping the data from websites is a solution. Web scraping has a bad reputation, as people think that you are stealing the data. It is unethical in my eyes in case you build up a competitive product with the data. But what is about taking the labelled photos to teach a neural network? I think this is OK. You might give the company even something valuable back or push their business with your data product.

I got my hands dirty over the last years to learn data science and to build up my data products. In this time I scraped data from many websites and learned a lot. I want to share the most important lessons with you:

### 1. Check the Legal Stuff

Before you scrape a website __ALWAYS__ check the [Terms of Service](https://en.wikipedia.org/wiki/Terms_of_service) of the site and their [robots.txt](https://en.wikipedia.org/wiki/Robots_exclusion_standard), which content you are allowed to fetch and which not. This saves you a lot of trouble and the risk of getting sued. Remember that get the data with scraping is a dangerous grey zone. Do you want to get sued by Google or Amazon?

### 2. Invest time for technical research

It's worth to invest time before start scraping. Sometimes you don't need to scrape/parse the raw HTML files as there exists an internal REST API under the hood. Modern webpages use JavaScript Frameworks which use [REST](https://en.wikipedia.org/wiki/Representational_state_transfer) APIs. You don't even not able to parse HTML as the logic is executed in the browser. In this scenario, you might need a framework like [Selenium](https://www.seleniumhq.org/), that uses a modern browser in headless mode that is executing the JavaScript code. In this lousy scenario you have the chance that there is RESTful API under the hood. These APIs just send JSON or XML back, which is even better than extracting the data from HTML. 

You also need to check what headers your browser sends to your website. The developer tools of the browsers e.g [Mozilla Browser Developer Tools](https://developer.mozilla.org/en-US/docs/Learn/Common_questions/What_are_browser_developer_tools) are your best friend here.

### 3. Use software design that reflects the business use case
Popular scraping frameworks like scrapy use a technical design for parsing. You feed them with a bunch of URLs which are then scraped and immediately parsed. I noticed that every scraping project is different and does not fit into this technical design. This often doesn't reflect the business use case why you are scraping. Instead, you have a business use like this: "Finding profitable niches on website XY". Often you don#T want to scrape the whole website

### 4. Dockerize your scraping application

### 4. Write a dump scraper first and the parser in the second step

### 4. Scrape every unique URL rarely

I try to scrape each URL only once. Or very very rarely!

You don't want to download the same resource over and over again, especially during development. We all have a kind of trial and error programming style. We write a piece of code and try if it works or not, do some bug fixing. When scraping a site and you try out different headers and URLs, and it's parameters you risk to get blocked already. Especially the big famous websites, just block you quicker than you think. The block is sometimes only for a few hours, but I am sure thye log the IP somewhere, which you should avoid. Keep this in mind:

- Don't send too many useless requests to a website. It's also a kind of respect to valuable data the site offers you. You don't want to get blocked or banned as said before
- Don't download massive amount of data in the beginning in multiple threads/processes. Try to download single resources until your scraping code works.

Implement a flag that you are able to force the resource can be downloaded when it is outdated. Outdated means for me several weeks rather than several hours. This depends for sure also what you want to do. If you need the data more often, try to scrape with more proxies.

### 5. Always keep the raw files (the raw oil!)

In 2. I wrote that you should try to avoid scraping the resource too often. Therefore it's essential always to keep the raw file once it is scraped. Usually, you are interested in data you weren't interested first. The raw files are like diamonds or raw oil and very valuable. So your steps:

Write a dump scraper and save the downloaded file
Always write the Parser in the second step, when you enough raw files.
Another advantage of having the raw that you can throw the extracted data away in case your parsing is terrible.

This way you can start your scraping project even before you know what data you interested in details. This saves a lot of time, as the dump raw data scraper is developed quickly.

### 6. Always use first class proxies for scraping

Use proxies already during development. This saves you a lot of troubles. My rule of thumb 10 proxies per scraping process/thread. Try to void free proxies list. The proxies on this list are often slow or already blocked.

### 7. Mimic browser headers

Set proper headers, when fetching a website. In my experience, most are the user agent header. I developed a library which takes a user agent randomly out of thousands.

### 8. Use a database for the raw files

Save the raw data in a database. I love MongoDB's GridFS for this. You have different advantages to this:

You are able to save metadata with the file (e.g. scraping date, search query). Example: Airbnb
You can have a history of raw files for a resource (precious)

### 9. Use a database for extracted data
Use a database for the scraped data. With databases you able to group data. MongoDB is an excellent choice here, as you don't need a scheme, you can just save the data as it is. Especially if you able to scrape from an internal API. From Apis you get JSON, and you are able to keep the JSON just like it is into the MongoDB.

### 10. Ensure data quality with Data Auditing techniques

Once you have a bunch of raw files and your parser is written do Exploratory Data Analysis to check data quality. Check for missing values. Do a univariate data analysis to check for outliers for each feature.

This step is essential and missed out by many people. You find the weak points of your parsing logic. You might get through several iterations to improve the parsing logic until you are happy.
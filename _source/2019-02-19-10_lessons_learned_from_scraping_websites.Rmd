---
layout: post
title: "10 Lessons Learned from Scraping Websites"
subtitle: "Valuable insights from retrieving data from many websites over the last years  I want to share with you"
categories:
  - data analysis
show_comments: true
tags: R, ggplot, dplyr, tidyverse, data vizualisation, data analysis, EDA
image: missing_values.jpg
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

> "Data is the new oil. It's valuable, but if unrefined it cannot really be used. It has to be changed into gas, plastic, chemicals, etc to create a valuable entity that drives profitable activity; so must data be broken down, analyzed for it to have value." 
-- <cite>Clive Humby</cite>

A frequent critism of Kaggle and MOOC (Massive Open Online Courses) is that the data retrieval part is not described, although it is an important part of data science.
You deal in the courses with data set that from the real world or pre cleaned.

Retrieving the data you need is an essential part as the data is most essential part of the journey. As data scientist either in company or as entrepreneur be able to get the data you need can make the difference to make an impact.

Unless you are working for an enterprise you don't have access to the data you need. As entrepreuneru

I got my hands dirty over the last years to learn data science and to build up data products. In this time I scraped data from many websites and learned my lessons. I want to share my insights with you.

### 1. Check the Legal Stuff
      
Before you scrape a website __ALWAYS__ check the _Terms of Service_ of the Site and the robots.txt, which content you are allowed to fetch and which not. This saves you a lot of trouble and the risk of getting sued. Remember that get the data with scraping is a dangerous grey zone. Do you want to get sued by Google or Amazon?

### 2. Invest time for a technical research

It's worth to invest time before start scraping. Sometimes you don't need to scrape the raw HTML files as there exist a internal Rest API under the hood. Modern webpages use JavaScript Frameworks which use Rest Apis. You don't even not able to parse HTML as the logic is executed in the browser. In these secenario you might need a framework like [Selenium](https://www.seleniumhq.org/). In this bad scenario you have the chance that there is restful APi under the hood. This APIs just send JSON back, which is much better than extracting the data from HTML.

### 3. Use software design that reflects the business use case

Popular scraping frameworks like [scrapy](https://scrapy.org/) use a technical design for parsing. You feed them with a bunch of URLs which are then scraped and immediatly parsed. I noticed that every scraping project is diffrent and does not fit into this technical desogn. This often doesn't reflect the business use case why you are scraping. Instead you have a business use like this: "Finding proftitable niches on website XY".  Often you don#T want to scrape the whole websiet

### 4. Write a dump scraper first and the parser in the second step

### 4. Scrape every resource just once

I try to scrape each resource just once. Just once!

You don't want to download the same resource over and over again, especially during development. We all have kind of trial and error style. So you write a bit of code and you test if it works or not. When scraping a site und you try out diffrent headers and URLs and it's parameters you risk to get blocked already. Especially the big famous website, just block you quicker than you think. The block is sometimes just for a few hours, but I am sure the log the IP somewhere, which you should you avoid.
Keep this in  ind:
 
    - You don't want to send too useless requests to a website. It's also a kind of respect to value data the website offers you.
    - You don't want to get blocked or banned as said before

Implement a flag that you are able to force the resource can be downloaded when it is outdated. Oudated means for me several weeks rather than several hours. This depends for sure also what you want to do. If you need the data more often, try to scrape with more proxies.
    
### 5. Always keep the raw files (the raw oil!)

In 2. I wrote that you should try to avoid scraping the resource to often. Therefore it's essential to always keep the raw file once it is scraped. Ofen you are interested in data you werent't interested first. The raw files are like diamonds or the raw oil and very valuable. So your steps:

1. Write a dump scraper and save the downloaded file
2. Always write the Parser in the second step, when you enough raw files.

Another advantage of having the raw that you can throw the extracted data away in case your parsing is bad.

This way you can start your scraping project even before you know what data you interested in details.  This saves a lot of time, as the dump raw data scraper is devoloped wuickly.

### 6. Always use first class proxies for scraping

Use proxies already during development. This saves you a lot of troubles. My rule of thumb 10 proxies per scraping process/thread. Try to void free proxies list. The proxies on these list are often slow or already blocked.

### 7. Mimic browser headers

Set proper headers, when fecthing a website. In my experience the most is the user agent header. I developed a library which takes randomly a user agent out of thousands.

### 8. Use a database for te raw files

Save the raw file in a database. I love MongoDB's GridFS for this. You have diffrent andvantages with this:

- You are able to save metadata with the file (e.g scraping date, search query). Example: Airbnb 
- You can have a history of raw files for a resource (very valuable)

### 9. Use a database for extracted data

Use a database for the scraped data. With databases you able to group data. MongoDB is very valuable here, as you don't need a scheme, you can just save the data as it is. Especially if you able to scrape from an internal API. From Apis you get JSON and you are able to save the Json just like it is into the MongoDB.

### 10. Ensure data quality with Data Auditing techniques

Once you have a bunch of raw file and your parser is written do Exploratory Data Analysis to check [data quality](https://en.wikipedia.org/wiki/Data_cleansing#Data_quality). [Check for missing values](http://jenslaufer.com/data/analysis/visualize_missing_values_with_ggplot.html). Do an univariate data analisis to check for outliers for each feature.

This step is essential and missed out by many people. You find the weak points of your parsing logic. You might got through several iterations to improve the parsing logic till you happy.
